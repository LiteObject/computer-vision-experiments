{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison: YOLO vs Detectron2\n",
    "\n",
    "In this notebook, we will compare the performance of different computer vision models, including YOLOv5, YOLOv8, and various Detectron2 architectures. We will evaluate their accuracy, speed, and other relevant metrics on a common dataset.\n",
    "\n",
    "## Models to Compare:\n",
    "- **YOLOv5**: Fast single-stage detector\n",
    "- **YOLOv8**: Latest YOLO with improved architecture  \n",
    "- **Faster R-CNN**: Two-stage detector from Detectron2\n",
    "- **RetinaNet**: Single-stage detector with focal loss\n",
    "- **Mask R-CNN**: Instance segmentation capable detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from models.detectron2_model import Detectron2Model\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# from models.yolo_v5 import YOLOv5  # Uncomment when available\n",
    "# from models.yolo_v8 import YOLOv8  # Uncomment when available\n",
    "# from data.dataset_loader import load_dataset  # Uncomment when available\n",
    "# from utils.metrics import calculate_metrics  # Uncomment when available\n",
    "\n",
    "# Load the dataset (placeholder for now)\n",
    "# dataset = load_dataset('path/to/dataset')\n",
    "\n",
    "# Initialize models (placeholder for YOLO models)\n",
    "# yolo_v5_model = YOLOv5(weights='path/to/yolov5/weights')\n",
    "# yolo_v8_model = YOLOv8(weights='path/to/yolov8/weights')\n",
    "\n",
    "# Evaluate models (placeholder)\n",
    "# yolo_v5_results = yolo_v5_model.evaluate(dataset)\n",
    "# yolo_v8_results = yolo_v8_model.evaluate(dataset)\n",
    "\n",
    "# Calculate metrics (placeholder)\n",
    "# yolo_v5_metrics = calculate_metrics(yolo_v5_results)\n",
    "# yolo_v8_metrics = calculate_metrics(yolo_v8_results)\n",
    "\n",
    "# Compare metrics (placeholder)\n",
    "# metrics_comparison = pd.DataFrame({\n",
    "#     'YOLOv5': yolo_v5_metrics,\n",
    "#     'YOLOv8': yolo_v8_metrics\n",
    "# })\n",
    "\n",
    "# Plot comparison (placeholder)\n",
    "# metrics_comparison.plot(kind='bar')\n",
    "# plt.title('Model Comparison')\n",
    "# plt.ylabel('Metrics')\n",
    "# plt.xlabel('Metric Type')\n",
    "# plt.show()\n",
    "\n",
    "print(\"Imports configured for Detectron2 model comparison\")\n",
    "print(\"YOLO imports are commented out - uncomment when models are available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all models for comparison\n",
    "print(\"Initializing models...\")\n",
    "\n",
    "# YOLO models (uncomment when weights are available)\n",
    "# yolo_v5_model = YOLOv5(weights='path/to/yolov5/weights')\n",
    "# yolo_v8_model = YOLOv8(weights='path/to/yolov8/weights')\n",
    "\n",
    "# Detectron2 models\n",
    "try:\n",
    "    detectron2_models = {\n",
    "        'Faster R-CNN': Detectron2Model('faster_rcnn', num_classes=80),\n",
    "        'RetinaNet': Detectron2Model('retinanet', num_classes=80),\n",
    "        'Mask R-CNN': Detectron2Model('mask_rcnn', num_classes=80),\n",
    "        'FCOS': Detectron2Model('fcos', num_classes=80)\n",
    "    }\n",
    "    print(\"Model initialization completed!\")\n",
    "\n",
    "    # Model information\n",
    "    for name, model in detectron2_models.items():\n",
    "        info = model.get_model_info()\n",
    "        print(f\"{name}: {info['architecture']} detector\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Detectron2 models: {e}\")\n",
    "    print(\"Make sure Detectron2 is properly installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmarking function\n",
    "def benchmark_model(model, test_images, model_name=\"Unknown\", num_runs=10):\n",
    "    \"\"\"Benchmark model performance on test images\"\"\"\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'inference_times': [],\n",
    "        'fps': 0,\n",
    "        'total_detections': 0,\n",
    "        'avg_confidence': 0\n",
    "    }\n",
    "\n",
    "    print(f\"Benchmarking {model_name}...\")\n",
    "\n",
    "    # Warm up runs\n",
    "    for _ in range(3):\n",
    "        _ = model.predict(test_images[0])\n",
    "\n",
    "    # Actual benchmark\n",
    "    all_detections = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(min(num_runs, len(test_images))):\n",
    "        img_start = time.time()\n",
    "        predictions = model.predict(test_images[i])\n",
    "        img_end = time.time()\n",
    "\n",
    "        results['inference_times'].append(img_end - img_start)\n",
    "\n",
    "        # Extract detection info (Detectron2 format)\n",
    "        if hasattr(predictions, '__getitem__') and 'instances' in predictions:\n",
    "            num_dets = len(predictions['instances'])\n",
    "            if num_dets > 0:\n",
    "                confidences = predictions['instances'].scores.cpu().numpy()\n",
    "                results['total_detections'] += num_dets\n",
    "                all_detections.extend(confidences)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate metrics\n",
    "    avg_inference_time = np.mean(results['inference_times'])\n",
    "    results['fps'] = 1.0 / avg_inference_time\n",
    "    results['avg_confidence'] = np.mean(\n",
    "        all_detections) if all_detections else 0\n",
    "    results['std_inference_time'] = np.std(results['inference_times'])\n",
    "\n",
    "    print(\n",
    "        f\"  Average inference time: {avg_inference_time:.3f}s Â± {results['std_inference_time']:.3f}s\")\n",
    "    print(f\"  FPS: {results['fps']:.1f}\")\n",
    "    print(f\"  Total detections: {results['total_detections']}\")\n",
    "    print(f\"  Average confidence: {results['avg_confidence']:.3f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "# For demonstration, create sample images (replace with your actual test dataset)\n",
    "def create_sample_images(num_images=5, height=480, width=640):\n",
    "    \"\"\"Create sample test images\"\"\"\n",
    "    images = []\n",
    "    for i in range(num_images):\n",
    "        # Create random image with some simple shapes for testing\n",
    "        img = np.random.randint(0, 255, (height, width, 3), dtype=np.uint8)\n",
    "\n",
    "        # Add some simple geometric shapes to make detection more interesting\n",
    "        cv2.circle(img, (width//4, height//4), 50, (255, 0, 0), -1)\n",
    "        cv2.rectangle(img, (width//2, height//2),\n",
    "                      (width//2 + 100, height//2 + 80), (0, 255, 0), -1)\n",
    "\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "# Create test images (replace with actual test dataset loading)\n",
    "test_images = create_sample_images(10)\n",
    "print(f\"Created {len(test_images)} test images for benchmarking\")\n",
    "\n",
    "# Or load actual test dataset (uncomment when available)\n",
    "# test_images = load_dataset('path/to/test/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarks on all models\n",
    "benchmark_results = {}\n",
    "\n",
    "# Benchmark Detectron2 models\n",
    "for name, model in detectron2_models.items():\n",
    "    results = benchmark_model(model, test_images, name,\n",
    "                              num_runs=len(test_images))\n",
    "    benchmark_results[name] = results\n",
    "\n",
    "# Benchmark YOLO models (uncomment when models are available)\n",
    "# yolo_v5_results = benchmark_model(yolo_v5_model, test_images, \"YOLOv5\")\n",
    "# yolo_v8_results = benchmark_model(yolo_v8_model, test_images, \"YOLOv8\")\n",
    "# benchmark_results[\"YOLOv5\"] = yolo_v5_results\n",
    "# benchmark_results[\"YOLOv8\"] = yolo_v8_results\n",
    "\n",
    "print(\"\\\\nBenchmarking completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison results\n",
    "def plot_comparison_results(results):\n",
    "    \\\"\\\"\\\"Create comprehensive comparison plots\\\"\\\"\\\"\n",
    "    model_names = list(results.keys())\n",
    "    fps_values = [results[name]['fps'] for name in model_names]\n",
    "    detection_counts = [results[name]['total_detections'] for name in model_names]\n",
    "    avg_confidences = [results[name]['avg_confidence'] for name in model_names]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Colors for bars\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold', 'orange', 'purple'][:len(model_names)]\n",
    "    \n",
    "    # 1. FPS Comparison\n",
    "    bars1 = ax1.bar(model_names, fps_values, color=colors)\n",
    "    ax1.set_ylabel('Frames Per Second (FPS)')\n",
    "    ax1.set_title('Inference Speed Comparison')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, fps in zip(bars1, fps_values):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                 f'{fps:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Detection Count Comparison\n",
    "    bars2 = ax2.bar(model_names, detection_counts, color=colors)\n",
    "    ax2.set_ylabel('Total Detections')\n",
    "    ax2.set_title('Detection Count Comparison')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, count in zip(bars2, detection_counts):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                 f'{count}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Average Confidence Comparison\n",
    "    bars3 = ax3.bar(model_names, avg_confidences, color=colors)\n",
    "    ax3.set_ylabel('Average Confidence Score')\n",
    "    ax3.set_title('Detection Confidence Comparison')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.set_ylim(0, 1)\n",
    "    \n",
    "    for bar, conf in zip(bars3, avg_confidences):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                 f'{conf:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Speed vs Accuracy Scatter Plot\n",
    "    ax4.scatter(fps_values, avg_confidences, c=colors[:len(model_names)], s=100, alpha=0.7)\n",
    "    ax4.set_xlabel('FPS (Speed)')\n",
    "    ax4.set_ylabel('Average Confidence (Accuracy Proxy)')\n",
    "    ax4.set_title('Speed vs Accuracy Trade-off')\n",
    "    \n",
    "    # Add model name labels\n",
    "    for i, name in enumerate(model_names):\n",
    "        ax4.annotate(name, (fps_values[i], avg_confidences[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the results\n",
    "plot_comparison_results(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we compared the performance of YOLOv5 and YOLOv8 on a common dataset. The results indicate that [insert findings here]. Further analysis can be conducted to explore the strengths and weaknesses of each model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
